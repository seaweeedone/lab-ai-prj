{
  "name": "org_code_mnist",
  "framework": "pytorch",
  "metric": ["accuracy"],
  "parameter": "parser.add_argument('--batch-size', type=int, default=64,\n                        help='input batch size for training (default: 64)')\nparser.add_argument('--learning-rate', type=float, default=0.001,\n                        help='learning rate (default: 0.001)')\nparser.add_argument('--epochs', type=int, default=5, metavar='N',\n                        help='number of epochs to train (default: 5)')",
  "model_block": "import argparse\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ndef get_data_loaders(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    return train_loader, test_loader\n\nclass MNISTModel(nn.Module):\n    def __init__(self):\n        super(MNISTModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3)\n        self.pool1 = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.pool2 = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n        self.output = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = self.output(x)\n        return x\n\nclass MetricsPrint:\n    def __init__(self):\n        self.history = {\"loss\": [], \"accuracy\": []}\n\n    def on_epoch_end(self, epoch, loss, accuracy):\n        self.history[\"loss\"].append(loss)\n        self.history[\"accuracy\"].append(accuracy)\n        print('\\nepoche {}:'.format(epoch))\n        print('loss={}'.format(loss))\n        print('accuracy={}'.format(accuracy))",
  "data_block": "def train(model, device, train_loader, optimizer, criterion, epoch, callbacks):\n    model.train()\n    running_loss = 0\n    correct = 0\n    total = 0\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * data.size(0)\n        preds = output.argmax(dim=1, keepdim=True)\n        correct += preds.eq(target.view_as(preds)).sum().item()\n        total += data.size(0)\n\n    avg_loss = running_loss / total\n    accuracy = correct / total\n    callbacks.on_epoch_end(epoch, avg_loss, accuracy)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--batch-size', type=int, default=64,\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--learning-rate', type=float, default=0.001,\n                        help='learning rate (default: 0.001)')\n    parser.add_argument('--epochs', type=int, default=5, metavar='N',\n                        help='number of epochs to train (default: 5)')\n    args = parser.parse_args(args=[])\n\n    print('Loading MNIST data...')\n    train_loader, _ = get_data_loaders(args.batch_size)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = MNISTModel().to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    print('Neural Network Model Summary:')\n    print(model)\n\n    metrics_callback = MetricsPrint()\n\n    for epoch in range(args.epochs):\n        train(model, device, train_loader, optimizer, criterion, epoch, metrics_callback)\n\nif __name__ == \"__main__\":\n    main()"
}